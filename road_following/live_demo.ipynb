{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following - Live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use model we trained to move jetBot smoothly on track. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that you have already downloaded ``best_steering_model_xy.pth`` to work station as instructed in \"train_model.ipynb\" notebook. Now, you should upload model file to JetBot in to this notebooks's directory. Once that's finished there should be a file named ``best_steering_model_xy.pth`` in this notebook's directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please make sure the file has uploaded fully before calling the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(512, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the trained weights from the ``best_steering_model_xy.pth`` file that you uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_steering_model_xy_best_without_normalization.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)\n",
    "model = model.eval().half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pre-Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded our model, but there's a slight issue. The format that we trained our model doesnt exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\n",
    "\n",
    "1. Convert from HWC layout to CHW layout\n",
    "2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "3. Transfer the data from CPU memory to GPU memory\n",
    "4. Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "Now, let's start and display our camera. You should be pretty familiar with this by now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b284d4cb6f034a09a975d7aeb5e40889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera()\n",
    "\n",
    "image_widget = ipywidgets.Image()\n",
    "\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define sliders to control JetBot\n",
    "> Note: We have initialize the slider values for best known configurations, however these might not work for your dataset, therefore please increase or decrease the sliders according to your setup and environment\n",
    "\n",
    "1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \n",
    "2. Steering Gain Control (steering_gain_sloder): If you see JetBot is woblling, you need to reduce ``steering_gain_slider`` till it is smooth\n",
    "3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\n",
    "\n",
    "> Note: You should play around above mentioned sliders with lower speed to get smooth JetBot road following behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9954e2798394e738e523a6cc241b57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='speed gain', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2991855e429422eb1b601ad6a416cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.2, description='steering gain', max=1.0, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f853c072ce6b41c7b21043f2ded9640b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering kd', max=0.5, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f44790b34f4414a1556f0f858784f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering bias', max=0.3, min=-0.3, step=0.01)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.2, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's display some sliders that will let us see what JetBot is thinking.  The x and y sliders will display the predicted x, y values.\n",
    "\n",
    "The steering slider will display our estimated steering value.  Please remember, this value isn't the actual angle of the target, but simply a value that is\n",
    "nearly proportional.  When the actual angle is ``0``, this will be zero, and it will increase / decrease with the actual angle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda2e62002754344906713d004260840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=0.0, description='y', max=1.0, orientation='vertical'), FloatSlider(value=0.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19adaa10650c4fc098b019717bbef012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='x', max=1.0, min=-1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a4bb22d92e41c8985764c5dd8ab2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='steering', max=1.0, min=-1.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. Compute the approximate steering value\n",
    "4. Control the motors using proportional / derivative control (PD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.5166e-01, 3.2187e-06, 4.8492e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last\n",
    "    image = change['new']\n",
    "    outputs = model(preprocess(image))\n",
    "#     x = xy[0]\n",
    "#     y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "#     x_slider.value = x\n",
    "#     y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "    \n",
    "#     angle = np.arctan2(x, y)\n",
    "#     pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "#     angle_last = angle\n",
    "    \n",
    "#     steering_slider.value = pid + steering_bias_slider.value\n",
    "    \n",
    "    out = F.softmax(outputs, dim = 1)\n",
    "    print(out)\n",
    "    print(out.max(1)[1])\n",
    "#     print(robot.left_motor)\n",
    "    robot.forward(speed = 0.1)\n",
    "#     robot.left_motor = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "#     robot.right_motor = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\n",
    "\n",
    "We accomplish that with the observe function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.5166e-01, 3.0994e-06, 4.8309e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5117e-01, 3.1590e-06, 4.8584e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5117e-01, 3.1590e-06, 4.8584e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5166e-01, 3.0994e-06, 4.8401e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5166e-01, 3.0398e-06, 4.8401e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5117e-01, 3.0994e-06, 4.8584e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5166e-01, 3.0994e-06, 4.8492e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5117e-01, 3.1590e-06, 4.8584e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5166e-01, 3.0994e-06, 4.8492e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5166e-01, 3.0398e-06, 4.8492e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5166e-01, 3.0994e-06, 4.8309e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5068e-01, 3.3379e-06, 4.9408e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3848e-01, 8.2850e-06, 6.1523e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4531e-01, 4.7684e-06, 5.4688e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4824e-01, 4.3511e-06, 5.1544e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5654e-01, 2.3842e-06, 4.3457e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3604e-01, 1.2338e-05, 6.4087e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4727e-01, 5.2452e-06, 5.2612e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.9927, 0.0000, 0.0071]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.0186e-01, 2.4116e-04, 9.8083e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5752e-01, 3.3975e-06, 4.2633e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.6963e-01, 3.5524e-04, 1.3000e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.9941, 0.0000, 0.0056]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.3667, 0.2419, 0.3914]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[9.7510e-01, 1.1921e-07, 2.4841e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.0264e-01, 2.4557e-04, 2.9688e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1055e-01, 1.5426e-04, 1.8945e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2422e-01, 1.0639e-04, 1.7554e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0127e-01, 1.4877e-04, 1.9849e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8320e-01, 2.1243e-04, 2.1643e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5244e-01, 2.2590e-04, 2.4719e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.3242e-01, 2.8396e-04, 2.6733e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.5908, 0.0338, 0.3752]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.9854, 0.0000, 0.0149]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.0771e-01, 2.1219e-05, 9.2346e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.8477e-01, 1.1736e-04, 1.1536e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.9502e-01, 4.7207e-05, 1.0492e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.0674e-01, 3.3379e-06, 9.3018e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.2188e-01, 8.3447e-07, 7.8247e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3750e-01, 1.1921e-06, 6.2683e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3994e-01, 1.4305e-06, 6.0211e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4092e-01, 1.6689e-06, 5.8899e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4141e-01, 1.3113e-06, 5.8441e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4043e-01, 1.3709e-06, 5.9540e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4043e-01, 1.2517e-06, 5.9418e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4141e-01, 1.1921e-06, 5.8563e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3799e-01, 1.4305e-06, 6.1981e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4043e-01, 1.2517e-06, 5.9540e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3848e-01, 1.3113e-06, 6.1523e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3799e-01, 1.3709e-06, 6.1890e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4336e-01, 1.0729e-06, 5.6763e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4287e-01, 1.0729e-06, 5.6976e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4385e-01, 1.0729e-06, 5.6244e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4189e-01, 1.1921e-06, 5.8228e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4189e-01, 1.1325e-06, 5.8228e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3750e-01, 1.3709e-06, 6.2439e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4043e-01, 1.3709e-06, 5.9540e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4043e-01, 1.3113e-06, 5.9540e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3848e-01, 1.2517e-06, 6.1768e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4092e-01, 2.8610e-06, 5.9204e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.3203e-01, 9.1672e-05, 1.6797e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.0625e-01, 8.2850e-06, 9.3689e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.2920e-01, 2.2054e-06, 7.0801e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3018e-01, 2.3842e-06, 7.0068e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3066e-01, 2.2054e-06, 6.9397e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3262e-01, 2.0266e-06, 6.7200e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3115e-01, 2.1458e-06, 6.8787e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3604e-01, 1.5497e-06, 6.4087e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3604e-01, 1.4305e-06, 6.3965e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3701e-01, 1.4901e-06, 6.2927e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.2383e-01, 3.6955e-06, 7.6111e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.3826, 0.2435, 0.3740]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.3750e-01, 5.3644e-06, 6.2317e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6045e-01, 8.3447e-07, 3.9795e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5361e-01, 1.6689e-06, 4.6478e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5898e-01, 9.5367e-07, 4.0924e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6045e-01, 7.7486e-07, 3.9429e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.3838e-01, 2.6584e-04, 1.6125e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4629e-01, 3.6359e-06, 5.3711e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5703e-01, 1.1921e-06, 4.3213e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8696e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8544e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8452e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8605e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8391e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8452e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 7.1526e-07, 3.8177e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8391e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.1526e-07, 3.8818e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.8971e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8696e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6045e-01, 8.3447e-07, 3.9490e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.8971e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.1526e-07, 3.8910e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8605e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8544e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.8910e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.1526e-07, 3.8818e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9124e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8757e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9185e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9062e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8605e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8696e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8757e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8605e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8696e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8605e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.1526e-07, 3.8910e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8544e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8605e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8452e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8391e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8452e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8696e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8391e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.7903e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7750e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7811e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.7903e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.7903e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.7903e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.7903e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7811e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.8025e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7750e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7750e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7750e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9185e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8391e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 7.1526e-07, 3.8330e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8605e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.8116e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 7.1526e-07, 3.8239e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 7.1526e-07, 3.8239e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.8025e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.7964e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 7.1526e-07, 3.8239e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8452e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 7.1526e-07, 3.8330e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9062e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8452e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.1526e-07, 3.8818e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8544e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9124e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9276e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6045e-01, 8.3447e-07, 3.9703e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5996e-01, 8.3447e-07, 4.0009e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5996e-01, 8.3447e-07, 4.0161e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6045e-01, 8.3447e-07, 3.9703e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6045e-01, 8.3447e-07, 3.9703e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5996e-01, 8.3447e-07, 3.9948e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.7486e-07, 3.9124e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8391e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6094e-01, 7.1526e-07, 3.8910e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6143e-01, 7.1526e-07, 3.8391e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.8116e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 7.1526e-07, 3.8330e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7750e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7537e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6289e-01, 5.9605e-07, 3.6987e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6240e-01, 6.5565e-07, 3.7598e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6289e-01, 5.9605e-07, 3.7048e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6289e-01, 5.9605e-07, 3.7109e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6289e-01, 5.9605e-07, 3.6896e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6191e-01, 6.5565e-07, 3.7903e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6289e-01, 5.9605e-07, 3.7109e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.5117e-01, 2.0862e-06, 4.8950e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.3779e-01, 7.3433e-04, 2.6172e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7324, 0.0008, 0.2671]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7305, 0.0008, 0.2690]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7383, 0.0008, 0.2612]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7329, 0.0008, 0.2664]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7217, 0.0010, 0.2771]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7241, 0.0009, 0.2751]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7314, 0.0009, 0.2678]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7192, 0.0010, 0.2795]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.4170e-01, 6.9094e-04, 2.5781e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.4072e-01, 6.5994e-04, 2.5854e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7134, 0.0010, 0.2856]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7402, 0.0008, 0.2590]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7354, 0.0008, 0.2637]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7349, 0.0008, 0.2644]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7197, 0.0009, 0.2795]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7285, 0.0009, 0.2705]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6855e-01, 4.9877e-04, 2.3096e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7432, 0.0007, 0.2559]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7388, 0.0008, 0.2605]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7085, 0.0011, 0.2905]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7280, 0.0009, 0.2712]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7266, 0.0009, 0.2727]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7329, 0.0009, 0.2661]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7314, 0.0009, 0.2676]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7246, 0.0009, 0.2744]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7217, 0.0009, 0.2771]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7246, 0.0009, 0.2744]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7300, 0.0009, 0.2690]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7363, 0.0008, 0.2627]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7280, 0.0009, 0.2710]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7383, 0.0008, 0.2610]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7246, 0.0009, 0.2747]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7251, 0.0009, 0.2739]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7378, 0.0009, 0.2610]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7227, 0.0009, 0.2764]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7109, 0.0010, 0.2878]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.7119, 0.0011, 0.2874]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.1533e-01, 6.9046e-04, 2.8369e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6025e-01, 5.4598e-04, 2.3938e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5977e-01, 5.1880e-04, 2.3962e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5049e-01, 5.6601e-04, 2.4915e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5537e-01, 5.6219e-04, 2.4426e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.3975e-01, 5.8842e-04, 2.5977e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5586e-01, 5.6601e-04, 2.4353e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.4902e-01, 5.6410e-04, 2.5049e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7686e-01, 4.7469e-04, 2.2253e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6562e-01, 4.8351e-04, 2.3413e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5830e-01, 5.0402e-04, 2.4121e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7100e-01, 4.5323e-04, 2.2839e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7393e-01, 4.4084e-04, 2.2546e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7295e-01, 4.6682e-04, 2.2656e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5977e-01, 4.9496e-04, 2.3975e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6709e-01, 4.5967e-04, 2.3242e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6660e-01, 4.9067e-04, 2.3291e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6709e-01, 4.7421e-04, 2.3242e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6172e-01, 4.9162e-04, 2.3767e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.4707e-01, 5.4550e-04, 2.5220e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6123e-01, 5.1498e-04, 2.3828e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6611e-01, 4.6921e-04, 2.3328e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7979e-01, 4.2033e-04, 2.1997e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6465e-01, 4.8399e-04, 2.3486e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7588e-01, 4.3869e-04, 2.2339e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6221e-01, 4.6325e-04, 2.3706e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5781e-01, 4.8447e-04, 2.4158e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7051e-01, 4.2939e-04, 2.2913e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7148e-01, 4.3941e-04, 2.2803e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7344e-01, 4.2105e-04, 2.2632e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7148e-01, 4.2677e-04, 2.2791e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6855e-01, 4.1866e-04, 2.3083e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6953e-01, 4.3058e-04, 2.3010e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6465e-01, 4.4751e-04, 2.3486e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6123e-01, 4.7159e-04, 2.3840e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6416e-01, 4.5681e-04, 2.3547e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.6807e-01, 4.4847e-04, 2.3169e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9053e-01, 3.5691e-04, 2.0886e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7441e-01, 4.1914e-04, 2.2534e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8418e-01, 3.9721e-04, 2.1545e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8564e-01, 3.7599e-04, 2.1411e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9004e-01, 3.5667e-04, 2.0935e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8809e-01, 3.6907e-04, 2.1167e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9004e-01, 3.4308e-04, 2.0959e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8711e-01, 3.6502e-04, 2.1265e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8662e-01, 3.6502e-04, 2.1277e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7344e-01, 3.9625e-04, 2.2632e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8467e-01, 3.6407e-04, 2.1497e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8760e-01, 3.5834e-04, 2.1204e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8857e-01, 3.4976e-04, 2.1106e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7100e-01, 4.3154e-04, 2.2852e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7734e-01, 3.9220e-04, 2.2241e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8906e-01, 3.4118e-04, 2.1069e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9590e-01, 3.2830e-04, 2.0386e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8467e-01, 3.6836e-04, 2.1472e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7490e-01, 4.0340e-04, 2.2485e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7393e-01, 3.9124e-04, 2.2546e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7539e-01, 3.5977e-04, 2.2412e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8760e-01, 3.5000e-04, 2.1216e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7979e-01, 3.8433e-04, 2.1997e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8613e-01, 3.4070e-04, 2.1350e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7344e-01, 3.7742e-04, 2.2620e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7246e-01, 3.8075e-04, 2.2705e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7686e-01, 3.7861e-04, 2.2266e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7002e-01, 4.1032e-04, 2.2974e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8418e-01, 3.6383e-04, 2.1545e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8906e-01, 3.2115e-04, 2.1033e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9541e-01, 3.2234e-04, 2.0447e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9443e-01, 3.1400e-04, 2.0520e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9297e-01, 3.0684e-04, 2.0667e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8809e-01, 3.2759e-04, 2.1167e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9395e-01, 3.1209e-04, 2.0569e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8955e-01, 3.2139e-04, 2.1008e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9736e-01, 3.0351e-04, 2.0251e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0078e-01, 2.8419e-04, 1.9897e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0566e-01, 2.6298e-04, 1.9385e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7881e-01, 3.6216e-04, 2.2058e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9883e-01, 2.9039e-04, 2.0068e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9785e-01, 2.8777e-04, 2.0190e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.7148e-01, 3.9005e-04, 2.2827e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9980e-01, 3.0565e-04, 2.0007e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.8955e-01, 3.1710e-04, 2.0984e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9346e-01, 2.9588e-04, 2.0605e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9346e-01, 2.8610e-04, 2.0630e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9932e-01, 2.9397e-04, 2.0032e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9834e-01, 2.8563e-04, 2.0154e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9541e-01, 2.9874e-04, 2.0447e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0176e-01, 2.7275e-04, 1.9800e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0615e-01, 2.5558e-04, 1.9373e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0566e-01, 2.5678e-04, 1.9434e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9688e-01, 2.9135e-04, 2.0300e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0273e-01, 2.7180e-04, 1.9727e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0322e-01, 2.5797e-04, 1.9666e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0127e-01, 2.6107e-04, 1.9836e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0322e-01, 2.5511e-04, 1.9666e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9492e-01, 2.8324e-04, 2.0471e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0273e-01, 2.6298e-04, 1.9727e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0908e-01, 2.3580e-04, 1.9080e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1201e-01, 2.3854e-04, 1.8787e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0420e-01, 2.6345e-04, 1.9556e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0225e-01, 2.6441e-04, 1.9775e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0469e-01, 2.5463e-04, 1.9495e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1348e-01, 2.3341e-04, 1.8616e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0225e-01, 2.5535e-04, 1.9739e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0811e-01, 2.3818e-04, 1.9189e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1055e-01, 2.3901e-04, 1.8921e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0762e-01, 2.4724e-04, 1.9202e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9395e-01, 2.7633e-04, 2.0593e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1689e-01, 2.1517e-04, 1.8262e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0371e-01, 2.4462e-04, 1.9604e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0322e-01, 2.5368e-04, 1.9666e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0713e-01, 2.3890e-04, 1.9275e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0518e-01, 2.3520e-04, 1.9446e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0908e-01, 2.2995e-04, 1.9067e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1104e-01, 2.1982e-04, 1.8884e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1348e-01, 2.1541e-04, 1.8652e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0322e-01, 2.4438e-04, 1.9666e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0811e-01, 2.2244e-04, 1.9189e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1689e-01, 2.1505e-04, 1.8286e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1006e-01, 2.1744e-04, 1.8982e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1396e-01, 2.1482e-04, 1.8579e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0811e-01, 2.1327e-04, 1.9141e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1689e-01, 2.0278e-04, 1.8311e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0859e-01, 2.1672e-04, 1.9116e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1152e-01, 2.1493e-04, 1.8848e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0615e-01, 2.2817e-04, 1.9373e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1592e-01, 2.0814e-04, 1.8396e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1055e-01, 2.1589e-04, 1.8933e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1689e-01, 2.0170e-04, 1.8298e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2080e-01, 1.9443e-04, 1.7920e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1396e-01, 2.0421e-04, 1.8567e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.9785e-01, 2.4366e-04, 2.0190e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1201e-01, 2.1040e-04, 1.8799e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1396e-01, 2.0063e-04, 1.8567e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1836e-01, 1.9097e-04, 1.8140e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1641e-01, 1.9491e-04, 1.8347e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1445e-01, 2.0456e-04, 1.8542e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1543e-01, 1.8871e-04, 1.8445e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1152e-01, 2.0349e-04, 1.8811e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2129e-01, 1.8752e-04, 1.7834e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.0859e-01, 2.1458e-04, 1.9116e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2422e-01, 1.7297e-04, 1.7578e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1689e-01, 1.9693e-04, 1.8298e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1543e-01, 2.0289e-04, 1.8408e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1348e-01, 1.9813e-04, 1.8616e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1055e-01, 2.1124e-04, 1.8945e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[7.5732e-01, 3.9983e-04, 2.4207e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[0.4207, 0.0162, 0.5630]], device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([2], device='cuda:0')\n",
      "tensor([[9.2188e-01, 6.0797e-06, 7.7942e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.6289e-01, 5.9605e-07, 3.6987e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[9.4922e-01, 2.0862e-06, 5.0812e-02]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1445e-01, 1.8239e-04, 1.8542e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1250e-01, 2.0051e-04, 1.8750e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1982e-01, 1.8537e-04, 1.7993e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1445e-01, 1.9920e-04, 1.8518e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1494e-01, 1.8716e-04, 1.8469e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1738e-01, 1.8346e-04, 1.8225e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2373e-01, 1.6797e-04, 1.7590e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2422e-01, 1.6582e-04, 1.7554e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2959e-01, 1.4985e-04, 1.7053e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2764e-01, 1.6451e-04, 1.7212e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2568e-01, 1.6570e-04, 1.7444e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2422e-01, 1.6034e-04, 1.7554e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2959e-01, 1.5044e-04, 1.7029e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2812e-01, 1.6069e-04, 1.7188e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2910e-01, 1.5056e-04, 1.7090e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.3154e-01, 1.4532e-04, 1.6833e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.1689e-01, 1.8287e-04, 1.8298e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.3252e-01, 1.4317e-04, 1.6736e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.3301e-01, 1.4246e-04, 1.6711e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.2861e-01, 1.5807e-04, 1.7126e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n",
      "tensor([[8.3203e-01, 1.4281e-04, 1.6785e-01]], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SoftmaxBackward>)\n",
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \n",
    "\n",
    "You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "camera.unobserve(execute, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track follwing the road!!!\n",
    "\n",
    "If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the JetBot should get even better :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
